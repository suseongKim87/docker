# Docker Hadoop3 베이스 만들기
docker의 이미지를 만들기 위해서 최소한의 os image를 다운로드 받아야 합니다. (centos, ubuntu etc...)  
예제에서는 centos를 기반으로 하고 있기 때문에 centos 7 을 기반으로 하도록 하겠습니다.  
사실 centos에서 docker project를 하기위해서 버전업 까지 하게되었습니다... linux 서버 설치 관련해서는 다음에 작성하도록 하겠습니다.  

## 1. centOS 7 이미지 다운로드
먼저 centos 7의 기본이 되는 docker hadoop base image를 만들기 위해서는 먼저 centos image를 다운 받아야 합니다.  
```{.text}
docker hub에 있는 centos 관련 이미지를 확인해 볼 수 있습니다.
$ docker search centos
NAME                               DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED
centos                             The official build of CentOS.                   5133                [OK]
ansible/centos7-ansible            Ansible on Centos7                              119                                     [OK]
...
```
아래와 같이 다운로드를 할 경우, CentOS의 offcial image를 다운로드 합니다.  
```{.text}
$ docker pull centos
```

## 2. docker centOS 7 접근하기
일단 다운로드 받은 centOS를 "docker run" 명령어를 통해 실행하고, "docker ps" 명령어로 실행 된 이미지를 확인 합니다.
```{text}
$ docker run -itd centos /bin/bash
dee37ed8d9042bc8f21f9ef97e20b48d103ad79010be67d674f17ced207680d2

$ docker ps
CONTAINER ID        IMAGE        COMMAND             CREATED             STATUS              PORTS              NAMES
dee37ed8d904        centos      "/bin/bash"       10 seconds ago      Up 9 seconds                         eloquent_ishizaka
```
실행된 container를 확인하고, 실행을 합니다. 실행은 "docker exec" 명령어를 통해 실행합니다.
```{text}
$ docker exec -it dee37ed8d904 /bin/bash
[root@dee37ed8d904 /]#
```

## 3. 실행 된 centOS 에서 계정을 생성 하고, yum update 및 필수 라이브러리 Install
centOS를 설치하고 yum을 최신 버전으로 업데이트 하고, 설치에 필요한 package를 install 합니다.  
```{text}
$ yum update
$ yum install -y curl openssh-server openssh-clients rsync wget net-tools rdate

```

## 4. open-jdk 설치 
open-jdk는 oracle-jdk의 유료화로 요즘 많은 개발자들이 옮겨가고 있는 추세입니다.  
우리는 지금까지 jdk를 돈내고 쓴적이 없고, open-jdk가 상용화에 문제가 없으니, jdk는 open-jdk로 진행하였습니다.  
```{text}
yum install -y java-1.8.0-openjdk-devel.x86_64
```

## 5. generate the SSH rsa key 생성  
서버간의 통신이 가능하게 공개키를 설정하기 위해서 아래와 같은 작업을 해줍니다. 
```{text}
$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:9tyVXHZtMIA4XFpXAkmBRDmxr7SUcJxpVbs0G58uV4U root@af70f7e9c9ed
The key's randomart image is:
+---[RSA 2048]----+
|       ++BB*=o+  |
|       .BBo. o +.|
|      . Oo  = E B|
|       + o . B *o|
|        S . o * .|
|       + = . o . |
|        o o o o  |
|             o   |
|                 |
+----[SHA256]-----+
```
key 생성이 완료되면, key가 저장된 기본경로(.ssh)를 Permission 700, .ssh 하위 디렉토리에 Permission 600을 정의합니다.  
```{text}
$ chmod 700 /root/.ssh
$ chmod 600 /root/.ssh/*
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```
docker인 경우(systemctl 명령어 사용이 불가능 하기 때문에 /sbin/sshd 를 직접 실행해 줘야합니다.
```{text}
 /sbin/sshd <=== systemctl restart sshd.
Could not load host key: /etc/ssh/ssh_host_rsa_key
Could not load host key: /etc/ssh/ssh_host_ecdsa_key
Could not load host key: /etc/ssh/ssh_host_ed25519_key
sshd: no hostkeys available -- exiting.
```
이후, 위 3개의 파일에 ssh-keygen을 통해 해당경로에 key를 만들어 줍니다.  
```{text}  
$ ssh-keygen -q -N "" -t rsa -f /etc/ssh/ssh_host_rsa_key
$ ssh-keygen -q -N "" -t dsa -f /etc/ssh/ssh_host_dsa_key
$ ssh-keygen -q -N "" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key
```  
마지막으로 모든 키 타입의 존재하지 않는 호스트 키를 생성해줍니다.
```{text}  
$ ssh-keygen -A(?)
ssh-keygen: generating new host keys: RSA1 ED25519
$ /sbin/sshd 
```

## 6. zookeeper 설치 및 설정
설치에 필요한 파일들은 다운로드 받기 위해서 wget을 설치 했으니, wget 명령어를 통해 다운로드를 해보도록 하겠습니다.  
```{text}
$ wget http://mirror.apache-kr.org/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz
```
다운로드 받은 파일을 풀어주고, 추가적 버전변경이 생길 수 있으므로 symbolic link를 통해 접근 할 수 있도록 합니다.
```{text}
$ tar -zxf zookeeper-3.4.13.tar.gz
$ mv zookeeper-3.4.13 /home/hduser/
$ ln -s /home/hduser/zookeeper-3.4.13 /home/hduser/zookeeper
```
이후, data, logs 폴더를 외부 폴더에 만들어 줍니다. 외부 폴더로 만들어 주는 이유는, docker volume 설정 때문입니다.  
container의 특징상 휘발성 데이터를 가지게 되므로, 외부와 연결되는 폴더를 만들어 데이터를 유지할 수 있도록 하기 위해서 입니다.  
자세한 내용은 다음챕터에서 hadoop을 실행할 때 설명하도록 하겠습니다.  
```{text}
$ ln -s /repository/zookeeper/data /home/hduser/zookeeper/data
$ ln -s /repository/zookeeper/logs /home/hduser/zookeeper/logs
```
다음은 zookeeper 설정파일 입니다. 저는 2대의 namenode(active, standby)를 설정해서 사용합니다.  
설정에서 중요한 점은 대부분의 클러스터를 구성 할 때처럼 선출이랑는 개념이 들어가서 홀수로 구성하는것이 원칙적으로 생각합니다.  
```{text}
$ cp /home/hduser/zookeeper/conf/zoo_sample.cfg /home/hduser/zookeeper/conf/zoo.cfg
$ vi zoo.cfg

*** zoo.cfg 설정 ***
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
# dataDir=/tmp/zookeeper
# dataDir=/repository/zookeeper/data
dataDir=/home/hduser/zookeeper/data
# dataLogDir=/repository/zookeeper/logs
dataLogDir=/home/hduser/zookeeper/logs
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
# maxClientCnxns=60
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
# autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
# autopurge.purgeInterval=1
server.1=master:2888:3888
server.2=standby:2888:3888
server.3=slave1:2888:3888
```

## 7. hadoop 설치 및 설정
위에서 zookeeper를 설치한 것처럼 wget 명령어를 통해 다운로드, 압축풀기, symbolic link, mount 작업을 실행합니다.  
```{text}
$ wget http://apache.tt.co.kr/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz
$ tar -zxf hadoop-3.1.1.tar.gz
$ mv hadoop-3.1.1 /home/hduser/ 
$ ln -s /home/hduser/hadoop-3.1.1 /home/hduser/hadoop
$ mkdir /repository/hadoop
$ mkdir /repository/hadoop/tmp /repository/hadoop/dfs /repository/hadoop/logs
$ ln -s /repository/hadoop/tmp /home/hduser/hadoop/tmp
$ ln -s /repository/hadoop/dfs /home/hduser/hadoop/dfs
$ ln -s /repository/hadoop/logs /home/hduser/hadoop/logs
```
위와 같이 작업을 통해 기본적이 hadoop 설치는 끝났습니다. 이후, 나머지 hadoop 설정을 진행합니다.  
hadoop 설정 작업은 "core-site.xml", "hdfs-site.xml", "mapred-site.xml", "yarn-site.xml"을 변경해 주면 됩니다.  
먼저 namenode의 설정파일인 "core-site.xml" 을 설정해 보도록 하겠습니다.  
여러 설정들이 있는데, hadoop 사이트의 설정 정보를 기반으로 만들어 추가적으로 변경사항이 있으신 경우는, 아래 사이트로 이동하셔서 추가하시면 됩니다.  
(https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/core-default.xml)
```{text}
***core-site.xml***
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoopcluster</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hduser/hadoop/tmp</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>4096</value>
    </property>
</configuration>
```  
다음은 "hdfs-site.xml" 설정입니다.    
(https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)
```{text}
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
    </property>
    <property>
        <name>dfs.namenode.http-bind-host</name>
        <value>0.0.0.0</value>
    </property>
    <property>
        <name>dfs.nameservices</name>
        <value>hadoopcluster</value>
    </property>

    <!-- Configurations for NameNode -->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
        <description>defualt=268435456</description>
    </property>
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
        <description>datanodeCount(5) * 20</description>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hduser/hadoop/dfs/name</value>
    </property>

    <!-- Configurations for DataNode -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hduser/hadoop/dfs/data</value>
    </property>

    <!-- Configurations for JournalNode -->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/home/hduser/hadoop/dfs/journal</value>
    </property>

    <!-- HA configuration (namemode, standbynamenode, journalnode setting)-->
    <!-- only a maximum of two NameNodes may be configured per nameservice -->
    <property>
         <name>dfs.ha.namenodes.hadoopcluster</name>
         <value>nn1,nn2</value>
    </property>
    <property>
         <name>dfs.namenode.rpc-address.hadoopcluster.nn1</name>
         <value>master:8020</value>
    </property>
    <property>
         <name>dfs.namenode.rpc-address.hadoopcluster.nn2</name>
         <value>standby:8020</value>
    </property>
    <property>
         <name>dfs.namenode.http-address.hadoopcluster.nn1</name>
         <value>master:9870</value>
    </property>
    <property>
         <name>dfs.namenode.http-address.hadoopcluster.nn2</name>
         <value>standby:9870</value>
    </property>
    <property>
         <name>dfs.namenode.https-address.hadoopcluster.nn1</name>
         <value>master:9880</value>
    </property>
    <property>
         <name>dfs.namenode.https-address.hadoopcluster.nn2</name>
         <value>standby:9880</value>
    </property>
    <property>
         <name>dfs.namenode.shared.edits.dir</name>
         <value>qjournal://master:8485;standby:8485;slave1:8485/hadoopcluster</value>
    </property>
    <property>
         <name>dfs.client.failover.proxy.provider.hadoopcluster</name>
         <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <!-- Automatic failover configuration (zookeeper setting)-->
    <property>
         <name>dfs.ha.automatic-failover.enabled</name>
         <value>true</value>
    </property>
    <property>
         <name>ha.zookeeper.quorum</name>
         <value>master:2181,standby:2181,slave1:2181</value>
    </property>

    <!-- Fencing configuration -->
    <property>
         <name>dfs.ha.fencing.methods</name>
         <value>sshfence</value>
    </property>
    <property>
         <name>dfs.ha.fencing.ssh.private-key-files</name>
         <value>/root/.ssh/id_rsa</value>
    </property>

    <!-- Storage for edits' files -->
    <property>
         <name>dfs.datanode.max.xcievers</name>
         <value>4096</value>
    </property>
    <property>
         <name>dfs.support.append</name>
         <value>true</value>
    </property>
    <property>
         <name>dfs.webhdfs.enabled</name>
         <value>true</value>
    </property>
    <property>
         <name>dfs.namenode.backup.address</name>
         <value>0.0.0.0:50100</value>
    </property>
    <property>
         <name>dfs.namenode.backup.http-address</name>
         <value>0.0.0.0:50105</value>
    </property>
    <property>
         <name>dfs.datanode.address</name>
         <value>0.0.0.0:9866</value>
    </property>
    <property>
         <name>dfs.datanode.ipc.address</name>
         <value>0.0.0.0:9867</value>
    </property>
    <property>
         <name>dfs.datanode.http.address</name>
         <value>0.0.0.0:9864</value>
    </property>
    <property>
         <name>dfs.journalnode.rpc-address</name>
         <value>0.0.0.0:8485</value>
    </property>
    <property>
         <name>dfs.journalnode.http-address</name>
         <value>0.0.0.0:8480</value>
    </property>
</configuration>
```  

다음은 "mapred-site.xml" 설정입니다.    
(https://hadoop.apache.org/docs/r3.1.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)   
```{text}   
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

    <!-- Configurations for MapReduce Applications: -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>1536</value>
        <description>Larger resource limit for maps.</description>
    </property>
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx1024M</value>
        <description>Larger heap-size for child jvms of maps.</description>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>3072</value>
        <description>Larger resource limit for reduces.</description>
    </property>
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx2560M</value>
        <description>Larger heap-size for child jvms of reduces.</description>
    </property>
    <property>
        <name>mapreduce.task.io.sort.mb</name>
        <value>512</value>
        <description>Higher memory-limit while sorting data for efficiency.</description>
    </property>
    <property>
        <name>mapreduce.task.io.sort.factor</name>
        <value>100</value>
        <description>More streams merged at once while sorting files.</description>
    </property>
    <property>
        <name>mapreduce.reduce.shuffle.parallelcopies</name>
        <value>50</value>
        <description>Higher number of parallel copies run by reduces to fetch outputs from very large number of maps.</description>
    </property>
    <property>
        <name>mapreduce.cluster.local.dir</name>
        <value>/home/hduser/hadoop/tmp/mapred/local</value>
    </property>

    <!-- Configurations for MapReduce JobHistory Server: -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
        <description>Default port is 10020.</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
        <description>Default port is 19888.</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.intermediate-done-dir</name>
        <value>/home/hduser/hadoop/tmp/mapred/mr-history/tmp</value>
        <description>Directory where history files are written by MapReduce jobs.</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>/home/hduser/hadoop/tmp/mapred/mr-history/done</value>
        <description>Directory where history files are managed by the MR JobHistory Server.</description>
    </property>
</configuration>
```   
마지막으로, "yarn-site.xml" 설정입니다.    
(https://hadoop.apache.org/docs/r3.1.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)   
```{text}
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

<!-- Site specific YARN configuration properties -->
<!-- Configurations for ResourceManager: -->
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>master:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>master:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>master:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>master:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>master:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
        <description>1GB</description>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>24576</value>
        <description>24GB</description>
    </property>

<!-- Configurations for NodeManager: -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>24576</value>
        <description>24GB</description>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
        <description>(MemorySize)*2.1</description>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/home/hduser/hadoop/tmp/nm-local-dir</value>
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/home/hduser/hadoop/logs/yarn/userlogs</value>
        <description></description>
    </property>
    <property>
        <name>yarn.nodemanager.log.retain-seconds</name>
        <value>10800</value>
        <description>
           Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/home/hduser/hadoop/logs/yarn/nm</value>
        <description></description>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
        <value>/home/hduser/hadoop/logs/yarn/nm_suffix</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```  
이것으로 기본적이 hadoop3 베이스 만들기는 끝났습니다.  
간단하게 설치하고, 설정하는 방법만 봤는데요, 다음글에서는 실행하면서 나머지를 설명 하고자 합니다. 
